{
  
    
        "post0": {
            "title": "Read and write hyper files in Python",
            "content": "Tableau Hyper IO . A simple wrapper for the Tableau Hyper API . Click here to see the code on GitHub . Why was this made? . For a project I was working on I needed to read hyper files. I searched if a package already existed and found only the pandleau package, which only writes to hyper files but does not read them and also uses the older extract 2.0 API. Since I couldn&#39;t find any other package that met my needs I decided to make one myself, which has been a good learning experience. . Installation . You can install tableauhyperio using pip: . pip install tableauhyperio . This will also try downloading the Tableau hyper API, tqdm, pandas. . Example usage . import tableauhyperio as hio # Reading a regular hyper file df = hio.read_hyper(&quot;example.hyper&quot;) # Reading a hyper file with a custom schema df = hio.read_hyper(&quot;example.hyper&quot;, &quot;my_schema&quot;) # Writing a regular hyper file hio.to_hyper(df, &quot;example_output.hyper&quot;) # Writing a hyper file with a custom schema and custom table name hio.to_hyper(df, &quot;example_output.hyper&quot;, &quot;my_schema&quot;, &quot;my_table&quot;) .",
            "url": "https://datastorydesign.com/python/tableau/2020/06/17/read-and-write-hyper-files-in-python.html",
            "relUrl": "/python/tableau/2020/06/17/read-and-write-hyper-files-in-python.html",
            "date": " • Jun 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Easily anonymize your data",
            "content": "Let&#39;s imagine two scenarios: . You&#39;re hiring consultants to work on your data but need to anonymize it first | You created something great that you want to make into a template for other people who can&#39;t see the data | . How would you personally go about solving this? . Anonymize df . I&#39;ve been confronted with these scenarios multiple times and had a very ad hoc, quick and dirty, kind of approach to it. Starting from scratch every time, which meant saving time in the short run but losing time and quality in the long run. . When a colleague asked how complicated it would be to make a general-purpose tool for this kind of process, I saw that as an opportunity to finally do something about this and create a project for my public portfolio that is quite useful. . The result being Anonymize df, which is a python package and Alteryx macro that helps you quickly and easily generate realistically fake data from a Pandas DataFrame. . Where to get it . You can install it like you would any other package through pip: . pip install anonymizedf . You can also find it here: . Link to Python code . Link to Alteryx macro . Python usage . import pandas as pd from anonymizedf.anonymizedf import anonymize # Import the data df = pd.read_csv(&quot;https://query.data.world/s/shcktxndtu3ojonm46tb5udlz7sp3e&quot;) # Prepare the data to be anonymized an = anonymize(df) . # Example 1 - just updates df an.fake_names(&quot;Customer Name&quot;) an.fake_ids(&quot;Customer ID&quot;) an.fake_whole_numbers(&quot;Loyalty Reward Points&quot;) an.fake_categories(&quot;Segment&quot;) an.fake_dates(&quot;Date&quot;) an.fake_decimal_numbers(&quot;Fraction&quot;) df.head() . Customer ID Customer Name Loyalty Reward Points Segment Date Fraction Fake_Customer Name Fake_Customer ID Fake_Loyalty Reward Points Fake_Segment Fake_Date Fake_Fraction . 0 AA-10315 | Alex Avila | 76 | Consumer | 01/01/2000 | 7.6 | Anne Briggs | FYKP18464993584790 | 715 | Segment 1 | 1988-02-21 | 81.70 | . 1 AA-10375 | Allen Armold | 369 | Consumer | 02/01/2000 | 36.9 | Kathryn Poole-Owens | KQLT34683822176548 | 305 | Segment 1 | 2012-01-21 | 49.64 | . 2 AA-10480 | Andrew Allen | 162 | Consumer | 03/01/2000 | 16.2 | Dorothy Knight-Smith | KEKQ23089097589905 | 723 | Segment 1 | 2017-12-05 | 45.49 | . 3 AA-10645 | Anna Andreadi | 803 | Consumer | 04/01/2000 | 80.3 | Dr. Dennis Lowe | JUFR80046496812327 | 503 | Segment 1 | 1993-08-19 | 43.85 | . 4 AB-10015 | Aaron Bergman | 935 | Consumer | 05/01/2000 | 93.5 | Joan Read | ZLEK68784141425071 | 103 | Segment 1 | 2018-10-26 | 65.30 | . # Example 2 - method chaining fake_df = ( an .fake_names(&quot;Customer Name&quot;, chaining=True) .fake_ids(&quot;Customer ID&quot;, chaining=True) .fake_whole_numbers(&quot;Loyalty Reward Points&quot;, chaining=True) .fake_categories(&quot;Segment&quot;, chaining=True) .fake_dates(&quot;Date&quot;, chaining=True) .fake_decimal_numbers(&quot;Fraction&quot;, chaining=True) .show_data_frame() ) fake_df.head() . Customer ID Customer Name Loyalty Reward Points Segment Date Fraction Fake_Customer Name Fake_Customer ID Fake_Loyalty Reward Points Fake_Segment Fake_Date Fake_Fraction . 0 AA-10315 | Alex Avila | 76 | Consumer | 01/01/2000 | 7.6 | Matthew Elliott | KQPQ33621304584922 | 62 | Segment 1 | 2011-05-24 | 96.96 | . 1 AA-10375 | Allen Armold | 369 | Consumer | 02/01/2000 | 36.9 | Lynne Harding | CLAA15849783691822 | 494 | Segment 1 | 2000-10-14 | 20.78 | . 2 AA-10480 | Andrew Allen | 162 | Consumer | 03/01/2000 | 16.2 | Dr. Molly Holmes | VTWU51877283324210 | 383 | Segment 1 | 1994-01-30 | 66.87 | . 3 AA-10645 | Anna Andreadi | 803 | Consumer | 04/01/2000 | 80.3 | Mr. Frederick Price | MVFX95041828905565 | 82 | Segment 1 | 2000-01-11 | 25.77 | . 4 AB-10015 | Aaron Bergman | 935 | Consumer | 05/01/2000 | 93.5 | Dean Davies | CRXZ11641101775380 | 786 | Segment 1 | 1996-08-19 | 38.32 | . # Example 4 - for multiple columns column_list = [&quot;Segment&quot;, &quot;Customer Name&quot;, &quot;Customer ID&quot;, &quot;Date&quot;] for column in column_list: an.fake_categories(column) df.head() . Customer ID Customer Name Loyalty Reward Points Segment Date Fraction Fake_Customer Name Fake_Customer ID Fake_Loyalty Reward Points Fake_Segment Fake_Date Fake_Fraction . 0 AA-10315 | Alex Avila | 76 | Consumer | 01/01/2000 | 7.6 | Customer Name 1 | Customer ID 1 | 62 | Segment 1 | Date 1 | 96.96 | . 1 AA-10375 | Allen Armold | 369 | Consumer | 02/01/2000 | 36.9 | Customer Name 2 | Customer ID 2 | 494 | Segment 1 | Date 2 | 20.78 | . 2 AA-10480 | Andrew Allen | 162 | Consumer | 03/01/2000 | 16.2 | Customer Name 3 | Customer ID 3 | 383 | Segment 1 | Date 3 | 66.87 | . 3 AA-10645 | Anna Andreadi | 803 | Consumer | 04/01/2000 | 80.3 | Customer Name 4 | Customer ID 4 | 82 | Segment 1 | Date 4 | 25.77 | . 4 AB-10015 | Aaron Bergman | 935 | Consumer | 05/01/2000 | 93.5 | Customer Name 5 | Customer ID 5 | 786 | Segment 1 | Date 5 | 38.32 | . # Example 5 - grouping d2 = {&quot;category&quot;: [&quot;low&quot;, &quot;low&quot;, &quot;high&quot;, &quot;high&quot;], &quot;number&quot;: [0.1, 1, 10.1, 100.1]} df2 = pd.DataFrame(data=d2) an = anonymize(df2) df2.head() . category number . 0 low | 0.1 | . 1 low | 1.0 | . 2 high | 10.1 | . 3 high | 100.1 | . # Without grouping - relative relationships lost when generating fake data an.fake_decimal_numbers(&quot;number&quot;) df2.head() . category number Fake_number . 0 low | 0.1 | 86.03 | . 1 low | 1.0 | 64.72 | . 2 high | 10.1 | 68.66 | . 3 high | 100.1 | 13.41 | . # With grouping - relative relationships remain the same an.fake_decimal_numbers(&quot;number&quot;, &quot;category&quot;) df2.head() . Grouped by category . category number Fake_number . 0 low | 0.1 | 0.38 | . 1 low | 1.0 | 0.92 | . 2 high | 10.1 | 63.70 | . 3 high | 100.1 | 59.40 | . Alteryx usage . The Macro can be downloaded from the gallery and used just like any other macro . . The interface for using the macro should be quite straightforward, if not let me know ;) . . If you have any thoughts / comments feel free to let me know . Also let me know if you&#39;d be interested in a &quot;behind the scenes&quot; post / video about how this was created. .",
            "url": "https://datastorydesign.com/python/alteryx/2020/06/11/easily-anonymize-your-data.html",
            "relUrl": "/python/alteryx/2020/06/11/easily-anonymize-your-data.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Python for Alteryx users",
            "content": "Overview of tools . We will recreate this Alteryx workflow using Python . . Text input . import pandas as pd text_input = pd.DataFrame( {&quot;Person&quot;: [&quot;You&quot;, &quot;Me&quot;, &quot;Everyone else&quot;], &quot;Number&quot;: [1, 1, 99]} ) text_input . Person Number . 0 You | 1 | . 1 Me | 1 | . 2 Everyone else | 99 | . Input data . import pandas as pd input_data = pd.read_csv(&quot;https://query.data.world/s/whbjtalht4sigazly6hdnnb5ynqald&quot;) input_data.head() . Category City Country Customer Name Discount Number of Records Order Date Order ID Postal Code Manufacturer Product Name Profit Quantity Region Sales Segment Ship Date Ship Mode State Sub-Category . 0 Furniture | Henderson | United States | Claire Gute | 0.00 | 1 | 11/8/2017 | CA-2017-152156 | 42420.0 | Bush | Bush Somerset Collection Bookcase | 41.9136 | 2 | South | 261.9600 | Consumer | 11/11/2017 | Second Class | Kentucky | Bookcases | . 1 Furniture | Henderson | United States | Claire Gute | 0.00 | 1 | 11/8/2017 | CA-2017-152156 | 42420.0 | Hon | Hon Deluxe Fabric Upholstered Stacking Chairs,... | 219.5820 | 3 | South | 731.9400 | Consumer | 11/11/2017 | Second Class | Kentucky | Chairs | . 2 Office Supplies | Los Angeles | United States | Darrin Van Huff | 0.00 | 1 | 6/12/2017 | CA-2017-138688 | 90036.0 | Universal | Self-Adhesive Address Labels for Typewriters b... | 6.8714 | 2 | West | 14.6200 | Corporate | 6/16/2017 | Second Class | California | Labels | . 3 Furniture | Fort Lauderdale | United States | Sean O&#39;Donnell | 0.45 | 1 | 10/11/2016 | US-2016-108966 | 33311.0 | Bretford | Bretford CR4500 Series Slim Rectangular Table | -383.0310 | 5 | South | 957.5775 | Consumer | 10/18/2016 | Standard Class | Florida | Tables | . 4 Office Supplies | Fort Lauderdale | United States | Sean O&#39;Donnell | 0.20 | 1 | 10/11/2016 | US-2016-108966 | 33311.0 | Eldon | Eldon Fold &#39;N Roll Cart System | 2.5164 | 2 | South | 22.3680 | Consumer | 10/18/2016 | Standard Class | Florida | Storage | . Select tool . selected_data = ( input_data.drop(columns=[&quot;Ship Date&quot;, &quot;Ship Mode&quot;]) # Example of deselecting .loc[ :, [&quot;Order ID&quot;, &quot;Category&quot;, &quot;Sub-Category&quot;, &quot;Product Name&quot;, &quot;Sales&quot;, &quot;Quantity&quot;] ] # Example of selecting .rename(columns={&quot;Sub-Category&quot;: &quot;Sub-Cat&quot;, &quot;Product Name&quot;: &quot;Product&quot;}) .astype({&quot;Quantity&quot;: &quot;int64&quot;, &quot;Category&quot;: &quot;str&quot;}) ) selected_data.head() . Order ID Category Sub-Cat Product Sales Quantity . 0 CA-2017-152156 | Furniture | Bookcases | Bush Somerset Collection Bookcase | 261.9600 | 2 | . 1 CA-2017-152156 | Furniture | Chairs | Hon Deluxe Fabric Upholstered Stacking Chairs,... | 731.9400 | 3 | . 2 CA-2017-138688 | Office Supplies | Labels | Self-Adhesive Address Labels for Typewriters b... | 14.6200 | 2 | . 3 US-2016-108966 | Furniture | Tables | Bretford CR4500 Series Slim Rectangular Table | 957.5775 | 5 | . 4 US-2016-108966 | Office Supplies | Storage | Eldon Fold &#39;N Roll Cart System | 22.3680 | 2 | . Formula tool . selected_data[&quot;Total Sales&quot;] = selected_data[&quot;Sales&quot;] * selected_data[&quot;Quantity&quot;] selected_data.head() . Order ID Category Sub-Cat Product Sales Quantity Total Sales . 0 CA-2017-152156 | Furniture | Bookcases | Bush Somerset Collection Bookcase | 261.9600 | 2 | 523.9200 | . 1 CA-2017-152156 | Furniture | Chairs | Hon Deluxe Fabric Upholstered Stacking Chairs,... | 731.9400 | 3 | 2195.8200 | . 2 CA-2017-138688 | Office Supplies | Labels | Self-Adhesive Address Labels for Typewriters b... | 14.6200 | 2 | 29.2400 | . 3 US-2016-108966 | Furniture | Tables | Bretford CR4500 Series Slim Rectangular Table | 957.5775 | 5 | 4787.8875 | . 4 US-2016-108966 | Office Supplies | Storage | Eldon Fold &#39;N Roll Cart System | 22.3680 | 2 | 44.7360 | . Filter tool . filtered_data = selected_data[selected_data[&quot;Sales&quot;] &gt; 1000] # To get the false sales_under_1000 = selected_data[selected_data[&quot;Sales&quot;] &lt; 1000] filtered_data.head() . Order ID Category Sub-Cat Product Sales Quantity Total Sales . 10 CA-2015-115812 | Furniture | Tables | Chromcraft Rectangular Conference Tables | 1706.184 | 9 | 15355.656 | . 24 CA-2016-106320 | Furniture | Tables | Bretford CR4500 Series Slim Rectangular Table | 1044.630 | 3 | 3133.890 | . 27 US-2016-150630 | Furniture | Bookcases | Riverside Palais Royal Lawyers Bookcase, Royal... | 3083.430 | 7 | 21584.010 | . 35 CA-2017-117590 | Technology | Phones | GE 30524EE4 | 1097.544 | 7 | 7682.808 | . 54 CA-2017-105816 | Technology | Phones | AT&amp;T CL83451 4-Handset Telephone | 1029.950 | 5 | 5149.750 | . sales_under_1000.head() . Order ID Category Sub-Cat Product Sales Quantity Total Sales . 0 CA-2017-152156 | Furniture | Bookcases | Bush Somerset Collection Bookcase | 261.9600 | 2 | 523.9200 | . 1 CA-2017-152156 | Furniture | Chairs | Hon Deluxe Fabric Upholstered Stacking Chairs,... | 731.9400 | 3 | 2195.8200 | . 2 CA-2017-138688 | Office Supplies | Labels | Self-Adhesive Address Labels for Typewriters b... | 14.6200 | 2 | 29.2400 | . 3 US-2016-108966 | Furniture | Tables | Bretford CR4500 Series Slim Rectangular Table | 957.5775 | 5 | 4787.8875 | . 4 US-2016-108966 | Office Supplies | Storage | Eldon Fold &#39;N Roll Cart System | 22.3680 | 2 | 44.7360 | . Sort tool . sorted_data = filtered_data.sort_values(by=&quot;Sales&quot;, ascending=False) sorted_data.head() . Order ID Category Sub-Cat Product Sales Quantity Total Sales . 9941 CA-2015-145317 | Technology | Machines | Cisco TelePresence System EX90 Videoconferenci... | 22638.480 | 6 | 135830.880 | . 8093 CA-2017-118689 | Technology | Copiers | Canon imageCLASS 2200 Advanced Copier | 17499.950 | 5 | 87499.750 | . 4905 CA-2018-140151 | Technology | Copiers | Canon imageCLASS 2200 Advanced Copier | 13999.960 | 4 | 55999.840 | . 9707 CA-2018-127180 | Technology | Copiers | Canon imageCLASS 2200 Advanced Copier | 11199.968 | 4 | 44799.872 | . 5297 CA-2018-166709 | Technology | Copiers | Canon imageCLASS 2200 Advanced Copier | 10499.970 | 3 | 31499.910 | . Summarize tool . summarized_data = ( sorted_data.groupby([&quot;Category&quot;, &quot;Sub-Cat&quot;]) .agg({&quot;Sales&quot;: &quot;sum&quot;}) .rename(columns={&quot;Sales&quot;: &quot;Sales by Sub-Cat&quot;}) ).reset_index() summarized_data.head() . Category Sub-Cat Sales by Sub-Cat . 0 Furniture | Bookcases | 49521.8256 | . 1 Furniture | Chairs | 129547.8520 | . 2 Furniture | Furnishings | 5638.9320 | . 3 Furniture | Tables | 102125.0490 | . 4 Office Supplies | Appliances | 36742.7900 | . summarized_data_alternative = sorted_data.groupby([&quot;Category&quot;, &quot;Sub-Cat&quot;]).agg( Sales_by_Sub_Cat=pd.NamedAgg(column=&quot;Sales&quot;, aggfunc=sum) ).reset_index() summarized_data_alternative.head() . Category Sub-Cat Sales_by_Sub_Cat . 0 Furniture | Bookcases | 49521.8256 | . 1 Furniture | Chairs | 129547.8520 | . 2 Furniture | Furnishings | 5638.9320 | . 3 Furniture | Tables | 102125.0490 | . 4 Office Supplies | Appliances | 36742.7900 | . Join tool . joined_data = pd.merge( sorted_data, summarized_data, on=[&quot;Category&quot;, &quot;Sub-Cat&quot;], how=&quot;inner&quot; ) joined_data.head() . Order ID Category Sub-Cat Product Sales Quantity Total Sales Sales by Sub-Cat . 0 CA-2015-145317 | Technology | Machines | Cisco TelePresence System EX90 Videoconferenci... | 22638.480 | 6 | 135830.880 | 162022.19 | . 1 US-2017-107440 | Technology | Machines | 3D Systems Cube Printer, 2nd Generation, Magenta | 9099.930 | 7 | 63699.510 | 162022.19 | . 2 CA-2017-158841 | Technology | Machines | HP Designjet T520 Inkjet Large Format Printer ... | 8749.950 | 5 | 43749.750 | 162022.19 | . 3 CA-2015-139892 | Technology | Machines | Lexmark MX611dhe Monochrome Laser Printer | 8159.952 | 8 | 65279.616 | 162022.19 | . 4 US-2018-168116 | Technology | Machines | Cubify CubeX 3D Printer Triple Head Print | 7999.980 | 4 | 31999.920 | 162022.19 | . left_join = pd.merge( sorted_data, summarized_data, on=[&quot;Category&quot;, &quot;Sub-Cat&quot;], how=&quot;left&quot;, indicator=True ) left_join = left_join[left_join[&quot;_merge&quot;] == &quot;left_only&quot;] left_join.drop(columns=&quot;_merge&quot;, inplace=True) left_join.head() . Order ID Category Sub-Cat Product Sales Quantity Total Sales Sales by Sub-Cat . Union tool . unioned_data = joined_data.append(sales_under_1000) unioned_data.head() . Category Order ID Product Quantity Sales Sales by Sub-Cat Sub-Cat Total Sales . 0 Technology | CA-2015-145317 | Cisco TelePresence System EX90 Videoconferenci... | 6 | 22638.480 | 162022.19 | Machines | 135830.880 | . 1 Technology | US-2017-107440 | 3D Systems Cube Printer, 2nd Generation, Magenta | 7 | 9099.930 | 162022.19 | Machines | 63699.510 | . 2 Technology | CA-2017-158841 | HP Designjet T520 Inkjet Large Format Printer ... | 5 | 8749.950 | 162022.19 | Machines | 43749.750 | . 3 Technology | CA-2015-139892 | Lexmark MX611dhe Monochrome Laser Printer | 8 | 8159.952 | 162022.19 | Machines | 65279.616 | . 4 Technology | US-2018-168116 | Cubify CubeX 3D Printer Triple Head Print | 4 | 7999.980 | 162022.19 | Machines | 31999.920 | . Output tool . unioned_data.to_csv(&quot;Sales summary.csv&quot;, index=False) .",
            "url": "https://datastorydesign.com/alteryx/python/2020/05/24/python-for-alteryx-users.html",
            "relUrl": "/alteryx/python/2020/05/24/python-for-alteryx-users.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Converting an Alteryx tool into a Python function",
            "content": "A while ago I was doing some fuzzy matching and tried out a challenge to convert the typical fuzzy matching process in Alteryx into a pythonic way of doing it. . While I didn’t end up completing it, I was quite happy that I was able to complete one of the tools, the Alteryx make group tool. . . While I had used it several times in Alteryx, I didn’t really know how it worked. Well.. I knew the make group tool makes groups! Obviously :) . But how does it actually do it? . In situations like this I find the best place to start is reading the documentation, which in Alteryx’s case its best at look at the tool examples found here: . . After opening the example, we’re greeted with this very helpful explanation: . . From looking closely at this we can deduce that the make group tool is actually doing a few things: . Looking at each row and seeing all the possible connections (Luke is connected to Obi, Jabba is connected to Boba and so forth). . | Based on those connections it then makes a network graph where the nodes are a unique list of each person and the edges are the relationships between each person. . | Then establishing that there are two groups. . | Finally, a group name is selected and a dataframe is outputted with two columns. A column for the group name and a column for the person. . | Having understood the logic of what the tool is doing, its time to convert that logic into Python code. . Hopefully, the code is commented enough so you can understand what its doing, otherwise let me know ;) . &quot;&quot;&quot;Replicates the make group tool in Alteryx&quot;&quot;&quot; # Import necessary modules import networkx as nx import pandas as pd from itertools import repeat from networkx.algorithms import community def makegroups(df): &quot;&quot;&quot; Replicates the functionality of the make group tool in Alteryx Expects a pandas dataframe (&quot;df&quot;) with two columns. &quot;&quot;&quot; try: # Renames df columns df.columns = [&#39;a&#39;, &#39;b&#39;] # Converting the df to a list for network edges edgesList = df.values.tolist() # Converting the df to a dictionary for network nodes nodeDict = df.to_dict(&#39;list&#39;) # Creating an empty graph G = nx.Graph() # Adding nodes from both lists in the dictionary # Effectively a merge of the two lists G.add_nodes_from(nodeDict[&#39;a&#39;]) G.add_nodes_from(nodeDict[&#39;b&#39;]) # Adding the edges from the edges list G.add_edges_from(edgesList) # Generates groups # TODO check that size of smallest clique, here 2 is dynamic comm = list(community.k_clique_communities(G, 2)) # Splits the communities into two lists comm1 = list(comm[0]) comm2 = list(comm[1]) # Generates list of the group name # Replicates the group name to match community list length group1 = list(repeat(comm1[0], len(comm1))) group2 = list(repeat(comm2[0], len(comm2))) # combines each group name and community list into a dataframe dfGroup1 = pd.DataFrame(list(zip(group1, comm1)), columns=[&#39;Group&#39;, &#39;key&#39;]) dfGroup2 = pd.DataFrame(list(zip(group2, comm2)), columns=[&#39;Group&#39;, &#39;key&#39;]) # Combines both dataframes into the final dataframe df = dfGroup1.append(dfGroup2, ignore_index=True) print(&quot;Success: Groups created&quot;) return df except Exception as e: print(&quot;Error: Function - makegroups:&quot;, e) . Now we can call the function and get the groups . makegroups(df) . . You can find the Python file and a notebook file at: https://github.com/AlexFrid/MakeGroup .",
            "url": "https://datastorydesign.com/alteryx/python/2020/02/01/alteryx-to-python.html",
            "relUrl": "/alteryx/python/2020/02/01/alteryx-to-python.html",
            "date": " • Feb 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Finished is better than perfect",
            "content": "I have a problem with procrastination. More specifically, I have a problem with procrastination in my personal life. Which I find quite frustrating because I’m generally on top of things at work. . Because of that, I’ve told myself for a long time that I actually don’t have a problem with procrastination, I just don’t have time right now, I’m tired or whatever else. . Sometimes that is actually true, but after taking time this summer and Christmas holiday just to reflect about my life goals and what I actually want to do, I realized I have a big problem with procrastination. Take this blog for example, I’ve been meaning to get this site up and running since I bought the domain in the fall of 2018! . What eventually got me to write this blog post was a newsletter from James Clear called “3 ideas, 2 quotes, 1 question”, which I highly recommend you check out. . The August 29, 2019 newsletter talked about making the most of your life and had this idea which blew my mind: . “You know yourself mostly by your thoughts. Everyone else in the world knows you only by your actions. Remember this when you feel misunderstood. You have to do or say something for others to know how you feel.” . This really got me thinking about sharing more. Sharing more of my thoughts, what I’m learning and projects I’m doing. To do that however, I need to overcome my problem with procrastination. . Practically what that means is implementing the ideas from two books I bought last year: . Atomic Habits | I listened to this through audible this summer, it has great practical content, I just need to put it into practice. . The End of Procrastination | Funnily enough, this book has been sitting on my desk for a few months now and I’ve just read the first chapter, now its time to stop procrastinating on reading a book about how to stop procrastinating! :) . What does this all mean for you? . This means you can expect to see a lot more from me as I write about things I’m learning and projects I’m working on. . To start with, I’m adopting a new motto to help me overcome some perfectionistic tendencies: finished is better than perfect! .",
            "url": "https://datastorydesign.com/story/2020/01/18/finished-is-better-than-perfect.html",
            "relUrl": "/story/2020/01/18/finished-is-better-than-perfect.html",
            "date": " • Jan 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi, I’m Alexander, an infinitely curious guy who’s sharing lessons learned, projects built and other reflections on this website. .",
          "url": "https://datastorydesign.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "About/hire me",
          "content": "Looking for a data hero? . Home | About/hire me | Search | Tags | . Hi I&#39;m Alexander&lt;/br&gt; your friendly neighbourhood &lt;/br&gt; data hero . I&#39;m looking for a new super hero squad to join starting in October.&lt;/br&gt; If you&#39;re on the lookout for a data hero to help you fight dirty data, bureaucracy and inefficiency then get in touch! . . Download CV . Trusted By . My Super Powers . I can do magic! &lt;/br&gt;At least in the way Elon Musk describes &lt;/br&gt; “Engineering is the closest thing to magic that exists in the world.” . Data Visualisation . I can help you find the gems in your data through effective visualisation in either Tableau, Python, R or D3.js . Data Engineering . I can help you set up efficient data pipelines using Python, AWS and Airflow or Alteryx . Machine Learning . I can help you gain real value from ML by combining business expertise with technical expertise . “Always found ways to add value” . ...While working exclusively remote, Alexander was able to dive into our community of thousands and began teaching and learning. He joined the team during a transition period that resulted in the expectations of his role dramatically changing. He picked up on Cargill&#39;s scale and complexities quickly, and always found ways to add value. &lt;/br&gt;&lt;/br&gt; ...His coaching ability really shined when he spent time with dozens of our community members answering questions across a diverse range of maturity, technologies, data, businesses, and geographies. He was in his element teaching workshops to hundreds of people and answering any questions 1x1. &lt;/br&gt;&lt;/br&gt; It&#39;s rare to find someone with Alexander&#39;s breadth and depth of experience combined with the coaching behaviors. . Mitchell Grewer - Self-Service Analytics Lead . Read on LinkedIn “Quick solutions to the most puzzling challenges” . ...I have come to know Alexander as committed, smart, driven, and collaborative.&lt;/br&gt;&lt;/br&gt; He demonstrated his expertise by pushing Tableau to its full potential while working on client projects, and passion for knowledge sharing by giving Alteryx &amp; Tableau training to the BCG consulting staff. &lt;/br&gt;&lt;/br&gt; His out of box thinking leads to quick solutions to the most puzzling challenges. . Laleh Zangeneh - Lead Analyst - GAMMA at The Boston Consulting Group (BCG) . Read on LinkedIn “Went above and beyond to help” . Alex worked with us on a project which has been fundamental to our ways of working within the portfolio team.&lt;/br&gt;&lt;/br&gt; He was absolutely fantastic, always extremely responsive, accommodating, patient (!) and positive!&lt;/br&gt;&lt;/br&gt; He also on several occasions went above and beyond to help me with other data related work non-related to the project- absolutely brilliant “customer service”. . Kasia Joniec - Head of Portfolio Operations at HG . Read on LinkedIn × Design And Plan . . Core feature . The emailing module basically will speed up your email marketing operations while offering more subscriber control. . Do you need to build lists for your email campaigns? It just got easier with Evolo. . List building framework | Easy database browsing | User administration | Automate user signup | Quick formatting tools | Fast email checking | . REQUEST BACK × Search To Optimize . . Core feature . The emailing module basically will speed up your email marketing operations while offering more subscriber control. . Do you need to build lists for your email campaigns? It just got easier with Evolo. . List building framework | Easy database browsing | User administration | Automate user signup | Quick formatting tools | Fast email checking | . REQUEST BACK See Me in Action . In this 5 minute video, I talk about the creative process &lt;/br&gt; for making good analytics dashboards . Get in touch! . . Download CV You also reach me here . Copyright © 2020 DataStoryDesign - All rights reserved .",
          "url": "https://datastorydesign.com/about-hireme/",
          "relUrl": "/about-hireme/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}